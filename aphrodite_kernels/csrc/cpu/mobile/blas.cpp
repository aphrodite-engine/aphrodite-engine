#include "threading.hpp"
#include <arm_neon.h>
#include <algorithm>
#include <vector>
#include <cstring>
#include <cmath>

namespace aphrodite::mobile {

static inline size_t compute_linear_index(const size_t* coords,
                                          const size_t* strides, size_t ndim) {
  size_t index = 0;
  for (size_t i = 0; i < ndim; ++i) {
    index += coords[i] * strides[i];
  }
  return index;
}

static inline void increment_coords(size_t* coords, const size_t* shape,
                                    size_t ndim) {
  for (int i = ndim - 1; i >= 0; --i) {
    coords[i]++;
    if (coords[i] < shape[i]) {
      break;
    }
    coords[i] = 0;
  }
}

void add_int8(const int8_t* a, const int8_t* b, int8_t* output,
              size_t num_elements) {
  aphrodite::mobile::parallel_for(
      num_elements, aphrodite::mobile::Thresholds::ELEMENT_WISE,
      [&](size_t start_idx, size_t end_idx) {
        const size_t vectorized_end =
            ((end_idx - start_idx) / NEON_VECTOR_SIZE) * NEON_VECTOR_SIZE +
            start_idx;

        for (size_t i = start_idx; i < vectorized_end; i += NEON_VECTOR_SIZE) {
          int8x16_t a_vec = vld1q_s8(&a[i]);
          int8x16_t b_vec = vld1q_s8(&b[i]);

          int16x8_t a_low = vmovl_s8(vget_low_s8(a_vec));
          int16x8_t a_high = vmovl_s8(vget_high_s8(a_vec));
          int16x8_t b_low = vmovl_s8(vget_low_s8(b_vec));
          int16x8_t b_high = vmovl_s8(vget_high_s8(b_vec));

          int16x8_t result_low = vaddq_s16(a_low, b_low);
          int16x8_t result_high = vaddq_s16(a_high, b_high);

          int8x16_t result_vec =
              vcombine_s8(vqmovn_s16(result_low), vqmovn_s16(result_high));
          vst1q_s8(&output[i], result_vec);
        }

        for (size_t i = vectorized_end; i < end_idx; ++i) {
          int32_t sum = static_cast<int32_t>(a[i]) + static_cast<int32_t>(b[i]);
          output[i] = clamp_to_int8(sum);
        }
      });
}

void subtract_int8(const int8_t* a, const int8_t* b, int8_t* output,
                   size_t num_elements) {
  aphrodite::mobile::parallel_for(
      num_elements, aphrodite::mobile::Thresholds::ELEMENT_WISE,
      [&](size_t start_idx, size_t end_idx) {
        const size_t vectorized_end =
            ((end_idx - start_idx) / NEON_VECTOR_SIZE) * NEON_VECTOR_SIZE +
            start_idx;

        for (size_t i = start_idx; i < vectorized_end; i += NEON_VECTOR_SIZE) {
          int8x16_t a_vec = vld1q_s8(&a[i]);
          int8x16_t b_vec = vld1q_s8(&b[i]);

          int16x8_t a_low = vmovl_s8(vget_low_s8(a_vec));
          int16x8_t a_high = vmovl_s8(vget_high_s8(a_vec));
          int16x8_t b_low = vmovl_s8(vget_low_s8(b_vec));
          int16x8_t b_high = vmovl_s8(vget_high_s8(b_vec));

          int16x8_t result_low = vsubq_s16(a_low, b_low);
          int16x8_t result_high = vsubq_s16(a_high, b_high);

          int8x16_t result_vec =
              vcombine_s8(vqmovn_s16(result_low), vqmovn_s16(result_high));
          vst1q_s8(&output[i], result_vec);
        }

        for (size_t i = vectorized_end; i < end_idx; ++i) {
          int32_t diff =
              static_cast<int32_t>(a[i]) - static_cast<int32_t>(b[i]);
          output[i] = clamp_to_int8(diff);
        }
      });
}

void multiply_int8(const int8_t* a, const int8_t* b, int8_t* output,
                   size_t num_elements) {
  aphrodite::mobile::parallel_for(
      num_elements, aphrodite::mobile::Thresholds::ELEMENT_WISE,
      [&](size_t start_idx, size_t end_idx) {
        const size_t vectorized_end =
            ((end_idx - start_idx) / NEON_VECTOR_SIZE) * NEON_VECTOR_SIZE +
            start_idx;

        for (size_t i = start_idx; i < vectorized_end; i += NEON_VECTOR_SIZE) {
          int8x16_t a_vec = vld1q_s8(&a[i]);
          int8x16_t b_vec = vld1q_s8(&b[i]);

          int16x8_t a_low = vmovl_s8(vget_low_s8(a_vec));
          int16x8_t a_high = vmovl_s8(vget_high_s8(a_vec));
          int16x8_t b_low = vmovl_s8(vget_low_s8(b_vec));
          int16x8_t b_high = vmovl_s8(vget_high_s8(b_vec));

          int16x8_t result_low = vmulq_s16(a_low, b_low);
          int16x8_t result_high = vmulq_s16(a_high, b_high);

          int8x16_t result_vec =
              vcombine_s8(vqmovn_s16(result_low), vqmovn_s16(result_high));
          vst1q_s8(&output[i], result_vec);
        }

        for (size_t i = vectorized_end; i < end_idx; ++i) {
          int32_t product =
              static_cast<int32_t>(a[i]) * static_cast<int32_t>(b[i]);
          output[i] = clamp_to_int8(product);
        }
      });
}

void divide_int8(const int8_t* a, const int8_t* b, int8_t* output,
                 size_t num_elements) {
  aphrodite::mobile::parallel_for(
      num_elements, aphrodite::mobile::Thresholds::ELEMENT_WISE,
      [&](size_t start_idx, size_t end_idx) {
        for (size_t i = start_idx; i < end_idx; ++i) {
          if (b[i] == 0) {
            output[i] = (a[i] >= 0) ? 127 : -128;
          } else {
            float result = static_cast<float>(a[i]) / static_cast<float>(b[i]);
            output[i] = clamp_to_int8(result);
          }
        }
      });
}

void add_f32(const float* a, const float* b, float* output,
             size_t num_elements) {
  aphrodite::mobile::parallel_for(
      num_elements, aphrodite::mobile::Thresholds::ELEMENT_WISE,
      [&](size_t start_idx, size_t end_idx) {
        constexpr size_t SIMD_WIDTH = 4;
        const size_t vectorized_end =
            start_idx + ((end_idx - start_idx) / SIMD_WIDTH) * SIMD_WIDTH;

        for (size_t i = start_idx; i < vectorized_end; i += SIMD_WIDTH) {
          float32x4_t a_vec = vld1q_f32(&a[i]);
          float32x4_t b_vec = vld1q_f32(&b[i]);
          float32x4_t result_vec = vaddq_f32(a_vec, b_vec);
          vst1q_f32(&output[i], result_vec);
        }

        for (size_t i = vectorized_end; i < end_idx; ++i) {
          output[i] = a[i] + b[i];
        }
      });
}

void subtract_f32(const float* a, const float* b, float* output,
                  size_t num_elements) {
  aphrodite::mobile::parallel_for(
      num_elements, aphrodite::mobile::Thresholds::ELEMENT_WISE,
      [&](size_t start_idx, size_t end_idx) {
        constexpr size_t SIMD_WIDTH = 4;
        const size_t vectorized_end =
            start_idx + ((end_idx - start_idx) / SIMD_WIDTH) * SIMD_WIDTH;

        for (size_t i = start_idx; i < vectorized_end; i += SIMD_WIDTH) {
          float32x4_t a_vec = vld1q_f32(&a[i]);
          float32x4_t b_vec = vld1q_f32(&b[i]);
          float32x4_t result_vec = vsubq_f32(a_vec, b_vec);
          vst1q_f32(&output[i], result_vec);
        }

        for (size_t i = vectorized_end; i < end_idx; ++i) {
          output[i] = a[i] - b[i];
        }
      });
}

void multiply_f32(const float* a, const float* b, float* output,
                  size_t num_elements) {
  aphrodite::mobile::parallel_for(
      num_elements, aphrodite::mobile::Thresholds::ELEMENT_WISE,
      [&](size_t start_idx, size_t end_idx) {
        constexpr size_t SIMD_WIDTH = 4;
        const size_t vectorized_end =
            start_idx + ((end_idx - start_idx) / SIMD_WIDTH) * SIMD_WIDTH;

        for (size_t i = start_idx; i < vectorized_end; i += SIMD_WIDTH) {
          float32x4_t a_vec = vld1q_f32(&a[i]);
          float32x4_t b_vec = vld1q_f32(&b[i]);
          float32x4_t result_vec = vmulq_f32(a_vec, b_vec);
          vst1q_f32(&output[i], result_vec);
        }

        for (size_t i = vectorized_end; i < end_idx; ++i) {
          output[i] = a[i] * b[i];
        }
      });
}

void divide_f32(const float* a, const float* b, float* output,
                size_t num_elements) {
  aphrodite::mobile::parallel_for(
      num_elements, aphrodite::mobile::Thresholds::ELEMENT_WISE,
      [&](size_t start_idx, size_t end_idx) {
        constexpr size_t SIMD_WIDTH = 4;
        const size_t vectorized_end =
            start_idx + ((end_idx - start_idx) / SIMD_WIDTH) * SIMD_WIDTH;

        for (size_t i = start_idx; i < vectorized_end; i += SIMD_WIDTH) {
          float32x4_t a_vec = vld1q_f32(&a[i]);
          float32x4_t b_vec = vld1q_f32(&b[i]);
          float32x4_t result_vec = vdivq_f32(a_vec, b_vec);
          vst1q_f32(&output[i], result_vec);
        }

        for (size_t i = vectorized_end; i < end_idx; ++i) {
          output[i] = a[i] / b[i];
        }
      });
}

void add_broadcast_int8(const int8_t* a, const int8_t* b, int8_t* output,
                        const size_t* a_strides, const size_t* b_strides,
                        const size_t* output_shape, size_t ndim) {
  size_t total_elements = 1;
  for (size_t i = 0; i < ndim; ++i) {
    total_elements *= output_shape[i];
  }

  std::vector<size_t> coords(ndim, 0);

  for (size_t linear_idx = 0; linear_idx < total_elements; ++linear_idx) {
    size_t a_idx = compute_linear_index(coords.data(), a_strides, ndim);
    size_t b_idx = compute_linear_index(coords.data(), b_strides, ndim);

    output[linear_idx] = clamp_to_int8(static_cast<int32_t>(a[a_idx]) +
                                       static_cast<int32_t>(b[b_idx]));

    increment_coords(coords.data(), output_shape, ndim);
  }
}

void subtract_broadcast_int8(const int8_t* a, const int8_t* b, int8_t* output,
                             const size_t* a_strides, const size_t* b_strides,
                             const size_t* output_shape, size_t ndim) {
  size_t total_elements = 1;
  for (size_t i = 0; i < ndim; ++i) {
    total_elements *= output_shape[i];
  }

  std::vector<size_t> coords(ndim, 0);

  for (size_t linear_idx = 0; linear_idx < total_elements; ++linear_idx) {
    size_t a_idx = compute_linear_index(coords.data(), a_strides, ndim);
    size_t b_idx = compute_linear_index(coords.data(), b_strides, ndim);

    output[linear_idx] = clamp_to_int8(static_cast<int32_t>(a[a_idx]) -
                                       static_cast<int32_t>(b[b_idx]));

    increment_coords(coords.data(), output_shape, ndim);
  }
}

void multiply_broadcast_int8(const int8_t* a, const int8_t* b, int8_t* output,
                             const size_t* a_strides, const size_t* b_strides,
                             const size_t* output_shape, size_t ndim) {
  size_t total_elements = 1;
  for (size_t i = 0; i < ndim; ++i) {
    total_elements *= output_shape[i];
  }

  std::vector<size_t> coords(ndim, 0);

  for (size_t linear_idx = 0; linear_idx < total_elements; ++linear_idx) {
    size_t a_idx = compute_linear_index(coords.data(), a_strides, ndim);
    size_t b_idx = compute_linear_index(coords.data(), b_strides, ndim);

    output[linear_idx] = clamp_to_int8(static_cast<int32_t>(a[a_idx]) *
                                       static_cast<int32_t>(b[b_idx]));

    increment_coords(coords.data(), output_shape, ndim);
  }
}

void divide_broadcast_int8(const int8_t* a, const int8_t* b, int8_t* output,
                           const size_t* a_strides, const size_t* b_strides,
                           const size_t* output_shape, size_t ndim) {
  size_t total_elements = 1;
  for (size_t i = 0; i < ndim; ++i) {
    total_elements *= output_shape[i];
  }

  std::vector<size_t> coords(ndim, 0);

  for (size_t linear_idx = 0; linear_idx < total_elements; ++linear_idx) {
    size_t a_idx = compute_linear_index(coords.data(), a_strides, ndim);
    size_t b_idx = compute_linear_index(coords.data(), b_strides, ndim);

    int32_t b_val = static_cast<int32_t>(b[b_idx]);
    if (b_val == 0) {
      output[linear_idx] = 0;
    } else {
      output[linear_idx] =
          clamp_to_int8(static_cast<int32_t>(a[a_idx]) / b_val);
    }

    increment_coords(coords.data(), output_shape, ndim);
  }
}

void add_broadcast_f32(const float* a, const float* b, float* output,
                       const size_t* a_strides, const size_t* b_strides,
                       const size_t* output_shape, size_t ndim) {
  size_t total_elements = 1;
  for (size_t i = 0; i < ndim; ++i) {
    total_elements *= output_shape[i];
  }

  std::vector<size_t> coords(ndim, 0);

  for (size_t linear_idx = 0; linear_idx < total_elements; ++linear_idx) {
    size_t a_idx = compute_linear_index(coords.data(), a_strides, ndim);
    size_t b_idx = compute_linear_index(coords.data(), b_strides, ndim);

    output[linear_idx] = a[a_idx] + b[b_idx];

    increment_coords(coords.data(), output_shape, ndim);
  }
}

void subtract_broadcast_f32(const float* a, const float* b, float* output,
                            const size_t* a_strides, const size_t* b_strides,
                            const size_t* output_shape, size_t ndim) {
  size_t total_elements = 1;
  for (size_t i = 0; i < ndim; ++i) {
    total_elements *= output_shape[i];
  }

  std::vector<size_t> coords(ndim, 0);

  for (size_t linear_idx = 0; linear_idx < total_elements; ++linear_idx) {
    size_t a_idx = compute_linear_index(coords.data(), a_strides, ndim);
    size_t b_idx = compute_linear_index(coords.data(), b_strides, ndim);

    output[linear_idx] = a[a_idx] - b[b_idx];

    increment_coords(coords.data(), output_shape, ndim);
  }
}

void multiply_broadcast_f32(const float* a, const float* b, float* output,
                            const size_t* a_strides, const size_t* b_strides,
                            const size_t* output_shape, size_t ndim) {
  size_t total_elements = 1;
  for (size_t i = 0; i < ndim; ++i) {
    total_elements *= output_shape[i];
  }

  std::vector<size_t> coords(ndim, 0);

  for (size_t linear_idx = 0; linear_idx < total_elements; ++linear_idx) {
    size_t a_idx = compute_linear_index(coords.data(), a_strides, ndim);
    size_t b_idx = compute_linear_index(coords.data(), b_strides, ndim);

    output[linear_idx] = a[a_idx] * b[b_idx];

    increment_coords(coords.data(), output_shape, ndim);
  }
}

void divide_broadcast_f32(const float* a, const float* b, float* output,
                          const size_t* a_strides, const size_t* b_strides,
                          const size_t* output_shape, size_t ndim) {
  size_t total_elements = 1;
  for (size_t i = 0; i < ndim; ++i) {
    total_elements *= output_shape[i];
  }

  std::vector<size_t> coords(ndim, 0);

  for (size_t linear_idx = 0; linear_idx < total_elements; ++linear_idx) {
    size_t a_idx = compute_linear_index(coords.data(), a_strides, ndim);
    size_t b_idx = compute_linear_index(coords.data(), b_strides, ndim);

    output[linear_idx] = a[a_idx] / b[b_idx];

    increment_coords(coords.data(), output_shape, ndim);
  }
}

void add_f16(const __fp16* a, const __fp16* b, __fp16* output,
             size_t num_elements) {
  aphrodite::mobile::parallel_for(
      num_elements, aphrodite::mobile::Thresholds::ELEMENT_WISE,
      [&](size_t start_idx, size_t end_idx) {
        constexpr size_t SIMD_WIDTH = 8;
        const size_t vectorized_end =
            start_idx + ((end_idx - start_idx) / SIMD_WIDTH) * SIMD_WIDTH;

        for (size_t i = start_idx; i < vectorized_end; i += SIMD_WIDTH) {
          float16x8_t a_vec = vld1q_f16(&a[i]);
          float16x8_t b_vec = vld1q_f16(&b[i]);
          float16x8_t result_vec = vaddq_f16(a_vec, b_vec);
          vst1q_f16(&output[i], result_vec);
        }

        for (size_t i = vectorized_end; i < end_idx; ++i) {
          output[i] = a[i] + b[i];
        }
      });
}

void add_f16_clipped(const __fp16* a, const __fp16* b, __fp16* output,
                     size_t num_elements) {
  aphrodite::mobile::parallel_for(
      num_elements, aphrodite::mobile::Thresholds::ELEMENT_WISE,
      [&](size_t start_idx, size_t end_idx) {
        constexpr size_t SIMD_WIDTH = 8;
        const size_t vectorized_end =
            start_idx + ((end_idx - start_idx) / SIMD_WIDTH) * SIMD_WIDTH;

        constexpr float FP16_MAX = 65500.0f;
        const float32x4_t max_val = vdupq_n_f32(FP16_MAX);
        const float32x4_t min_val = vdupq_n_f32(-FP16_MAX);

        for (size_t i = start_idx; i < vectorized_end; i += SIMD_WIDTH) {
          float16x8_t a_vec = vld1q_f16(&a[i]);
          float16x8_t b_vec = vld1q_f16(&b[i]);

          float32x4_t a_low = vcvt_f32_f16(vget_low_f16(a_vec));
          float32x4_t a_high = vcvt_f32_f16(vget_high_f16(a_vec));
          float32x4_t b_low = vcvt_f32_f16(vget_low_f16(b_vec));
          float32x4_t b_high = vcvt_f32_f16(vget_high_f16(b_vec));

          float32x4_t result_low = vaddq_f32(a_low, b_low);
          float32x4_t result_high = vaddq_f32(a_high, b_high);

          result_low = vminq_f32(vmaxq_f32(result_low, min_val), max_val);
          result_high = vminq_f32(vmaxq_f32(result_high, min_val), max_val);

          float16x4_t result_low_f16 = vcvt_f16_f32(result_low);
          float16x4_t result_high_f16 = vcvt_f16_f32(result_high);
          float16x8_t result_vec =
              vcombine_f16(result_low_f16, result_high_f16);

          vst1q_f16(&output[i], result_vec);
        }

        for (size_t i = vectorized_end; i < end_idx; ++i) {
          float result = static_cast<float>(a[i]) + static_cast<float>(b[i]);
          result = std::fmin(std::fmax(result, -FP16_MAX), FP16_MAX);
          output[i] = static_cast<__fp16>(result);
        }
      });
}

void subtract_f16(const __fp16* a, const __fp16* b, __fp16* output,
                  size_t num_elements) {
  aphrodite::mobile::parallel_for(
      num_elements, aphrodite::mobile::Thresholds::ELEMENT_WISE,
      [&](size_t start_idx, size_t end_idx) {
        constexpr size_t SIMD_WIDTH = 8;
        const size_t vectorized_end =
            start_idx + ((end_idx - start_idx) / SIMD_WIDTH) * SIMD_WIDTH;

        for (size_t i = start_idx; i < vectorized_end; i += SIMD_WIDTH) {
          float16x8_t a_vec = vld1q_f16(&a[i]);
          float16x8_t b_vec = vld1q_f16(&b[i]);
          float16x8_t result_vec = vsubq_f16(a_vec, b_vec);
          vst1q_f16(&output[i], result_vec);
        }

        for (size_t i = vectorized_end; i < end_idx; ++i) {
          output[i] = a[i] - b[i];
        }
      });
}

void multiply_f16(const __fp16* a, const __fp16* b, __fp16* output,
                  size_t num_elements) {
  aphrodite::mobile::parallel_for(
      num_elements, aphrodite::mobile::Thresholds::ELEMENT_WISE,
      [&](size_t start_idx, size_t end_idx) {
        constexpr size_t SIMD_WIDTH = 8;
        const size_t vectorized_end =
            start_idx + ((end_idx - start_idx) / SIMD_WIDTH) * SIMD_WIDTH;

        for (size_t i = start_idx; i < vectorized_end; i += SIMD_WIDTH) {
          float16x8_t a_vec = vld1q_f16(&a[i]);
          float16x8_t b_vec = vld1q_f16(&b[i]);
          float16x8_t result_vec = vmulq_f16(a_vec, b_vec);
          vst1q_f16(&output[i], result_vec);
        }

        for (size_t i = vectorized_end; i < end_idx; ++i) {
          output[i] = a[i] * b[i];
        }
      });
}

void divide_f16(const __fp16* a, const __fp16* b, __fp16* output,
                size_t num_elements) {
  aphrodite::mobile::parallel_for(
      num_elements, aphrodite::mobile::Thresholds::ELEMENT_WISE,
      [&](size_t start_idx, size_t end_idx) {
        constexpr size_t SIMD_WIDTH = 8;
        const size_t vectorized_end =
            start_idx + ((end_idx - start_idx) / SIMD_WIDTH) * SIMD_WIDTH;

        for (size_t i = start_idx; i < vectorized_end; i += SIMD_WIDTH) {
          float16x8_t a_vec = vld1q_f16(&a[i]);
          float16x8_t b_vec = vld1q_f16(&b[i]);
          float16x8_t result_vec = vdivq_f16(a_vec, b_vec);
          vst1q_f16(&output[i], result_vec);
        }

        for (size_t i = vectorized_end; i < end_idx; ++i) {
          output[i] = a[i] / b[i];
        }
      });
}

void add_broadcast_f16(const __fp16* a, const __fp16* b, __fp16* output,
                       const size_t* a_strides, const size_t* b_strides,
                       const size_t* output_shape, size_t ndim) {
  size_t total_elements = 1;
  for (size_t i = 0; i < ndim; ++i) {
    total_elements *= output_shape[i];
  }

  std::vector<size_t> coords(ndim, 0);
  constexpr float FP16_MAX = 65500.0f;

  for (size_t linear_idx = 0; linear_idx < total_elements; ++linear_idx) {
    size_t a_idx = compute_linear_index(coords.data(), a_strides, ndim);
    size_t b_idx = compute_linear_index(coords.data(), b_strides, ndim);

    float result = static_cast<float>(a[a_idx]) + static_cast<float>(b[b_idx]);
    result = std::fmin(std::fmax(result, -FP16_MAX), FP16_MAX);
    output[linear_idx] = static_cast<__fp16>(result);

    increment_coords(coords.data(), output_shape, ndim);
  }
}

void subtract_broadcast_f16(const __fp16* a, const __fp16* b, __fp16* output,
                            const size_t* a_strides, const size_t* b_strides,
                            const size_t* output_shape, size_t ndim) {
  size_t total_elements = 1;
  for (size_t i = 0; i < ndim; ++i) {
    total_elements *= output_shape[i];
  }

  std::vector<size_t> coords(ndim, 0);

  for (size_t linear_idx = 0; linear_idx < total_elements; ++linear_idx) {
    size_t a_idx = compute_linear_index(coords.data(), a_strides, ndim);
    size_t b_idx = compute_linear_index(coords.data(), b_strides, ndim);

    output[linear_idx] = a[a_idx] - b[b_idx];

    increment_coords(coords.data(), output_shape, ndim);
  }
}

void multiply_broadcast_f16(const __fp16* a, const __fp16* b, __fp16* output,
                            const size_t* a_strides, const size_t* b_strides,
                            const size_t* output_shape, size_t ndim) {
  size_t total_elements = 1;
  for (size_t i = 0; i < ndim; ++i) {
    total_elements *= output_shape[i];
  }

  std::vector<size_t> coords(ndim, 0);

  for (size_t linear_idx = 0; linear_idx < total_elements; ++linear_idx) {
    size_t a_idx = compute_linear_index(coords.data(), a_strides, ndim);
    size_t b_idx = compute_linear_index(coords.data(), b_strides, ndim);

    output[linear_idx] = a[a_idx] * b[b_idx];

    increment_coords(coords.data(), output_shape, ndim);
  }
}

void divide_broadcast_f16(const __fp16* a, const __fp16* b, __fp16* output,
                          const size_t* a_strides, const size_t* b_strides,
                          const size_t* output_shape, size_t ndim) {
  size_t total_elements = 1;
  for (size_t i = 0; i < ndim; ++i) {
    total_elements *= output_shape[i];
  }

  std::vector<size_t> coords(ndim, 0);

  for (size_t linear_idx = 0; linear_idx < total_elements; ++linear_idx) {
    size_t a_idx = compute_linear_index(coords.data(), a_strides, ndim);
    size_t b_idx = compute_linear_index(coords.data(), b_strides, ndim);

    output[linear_idx] = a[a_idx] / b[b_idx];

    increment_coords(coords.data(), output_shape, ndim);
  }
}

void transpose_f32(const float* source, float* destination, const size_t* shape,
                   const size_t* permutation, size_t ndim, size_t start_idx,
                   size_t end_idx) {
  if (ndim == 2 && permutation[0] == 1 && permutation[1] == 0) {
    size_t num_rows = shape[0];
    size_t num_cols = shape[1];

    constexpr size_t THRESHOLD = 8192;
    constexpr size_t TILE_ROWS = 32;
    if (num_rows * num_cols >= THRESHOLD) {
      const size_t num_row_blocks = (num_rows + TILE_ROWS - 1) / TILE_ROWS;

      aphrodite::mobile::parallel_for(
          num_row_blocks, aphrodite::mobile::Thresholds::SCALAR_EXPENSIVE,
          [=](size_t start_block, size_t end_block) {
            for (size_t block_idx = start_block; block_idx < end_block;
                 ++block_idx) {
              size_t start_row = block_idx * TILE_ROWS;
              size_t end_row = std::min(start_row + TILE_ROWS, num_rows);

              transpose_2d_f32(source, destination, num_rows, num_cols,
                                      start_row, end_row);
            }
          });
    } else {
      cactus_transpose_2d_f32(source, destination, num_rows, num_cols, 0,
                              num_rows);
    }
  } else {
    for (size_t idx = start_idx; idx < end_idx; ++idx) {
      size_t src_idx = 0;
      size_t tmp_idx = idx;

      for (size_t i = 0; i < ndim; ++i) {
        size_t coord = tmp_idx % shape[permutation[ndim - 1 - i]];
        tmp_idx /= shape[permutation[ndim - 1 - i]];

        size_t stride = 1;
        for (size_t j = permutation[ndim - 1 - i] + 1; j < ndim; ++j) {
          stride *= shape[j];
        }
        src_idx += coord * stride;
      }

      destination[idx] = source[src_idx];
    }
  }
}

void concat_f32(const float* input1, const float* input2, float* output,
                const size_t* shape1, const size_t* shape2,
                const size_t* output_shape, size_t ndims, int axis) {
  if (axis < 0) axis += ndims;

  size_t outer_size = 1;
  for (size_t i = 0; i < static_cast<size_t>(axis); ++i) {
    outer_size *= output_shape[i];
  }

  size_t inner_size = 1;
  for (size_t i = axis + 1; i < ndims; ++i) {
    inner_size *= output_shape[i];
  }

  size_t axis_size1 = shape1[axis];
  size_t axis_size2 = shape2[axis];

  size_t input1_stride = axis_size1 * inner_size;
  size_t input2_stride = axis_size2 * inner_size;
  size_t output_stride = (axis_size1 + axis_size2) * inner_size;

  aphrodite::mobile::parallel_for(
      outer_size, aphrodite::mobile::Thresholds::ELEMENT_WISE,
      [&](size_t start, size_t end) {
        for (size_t outer = start; outer < end; ++outer) {
          const float* in1_ptr = input1 + outer * input1_stride;
          const float* in2_ptr = input2 + outer * input2_stride;
          float* out_ptr = output + outer * output_stride;

          size_t copy_size1 = axis_size1 * inner_size;
          std::memcpy(out_ptr, in1_ptr, copy_size1 * sizeof(float));

          size_t copy_size2 = axis_size2 * inner_size;
          std::memcpy(out_ptr + copy_size1, in2_ptr,
                      copy_size2 * sizeof(float));
        }
      });
}

void concat_f16(const __fp16* input1, const __fp16* input2, __fp16* output,
                const size_t* shape1, const size_t* shape2,
                const size_t* output_shape, size_t ndims, int axis) {
  if (axis < 0) axis += ndims;

  size_t outer_size = 1;
  for (size_t i = 0; i < static_cast<size_t>(axis); ++i) {
    outer_size *= output_shape[i];
  }

  size_t inner_size = 1;
  for (size_t i = axis + 1; i < ndims; ++i) {
    inner_size *= output_shape[i];
  }

  size_t axis_size1 = shape1[axis];
  size_t axis_size2 = shape2[axis];

  size_t input1_stride = axis_size1 * inner_size;
  size_t input2_stride = axis_size2 * inner_size;
  size_t output_stride = (axis_size1 + axis_size2) * inner_size;

  aphrodite::mobile::parallel_for(
      outer_size, aphrodite::mobile::Thresholds::ELEMENT_WISE,
      [&](size_t start, size_t end) {
        for (size_t outer = start; outer < end; ++outer) {
          const __fp16* in1_ptr = input1 + outer * input1_stride;
          const __fp16* in2_ptr = input2 + outer * input2_stride;
          __fp16* out_ptr = output + outer * output_stride;

          size_t copy_size1 = axis_size1 * inner_size;
          std::memcpy(out_ptr, in1_ptr, copy_size1 * sizeof(__fp16));

          size_t copy_size2 = axis_size2 * inner_size;
          std::memcpy(out_ptr + copy_size1, in2_ptr,
                      copy_size2 * sizeof(__fp16));
        }
      });
}

void concat_int8(const int8_t* input1, const int8_t* input2, int8_t* output,
                 const size_t* shape1, const size_t* shape2,
                 const size_t* output_shape, size_t ndims, int axis) {
  if (axis < 0) axis += ndims;

  size_t outer_size = 1;
  for (size_t i = 0; i < static_cast<size_t>(axis); ++i) {
    outer_size *= output_shape[i];
  }

  size_t inner_size = 1;
  for (size_t i = axis + 1; i < ndims; ++i) {
    inner_size *= output_shape[i];
  }

  size_t axis_size1 = shape1[axis];
  size_t axis_size2 = shape2[axis];

  size_t input1_stride = axis_size1 * inner_size;
  size_t input2_stride = axis_size2 * inner_size;
  size_t output_stride = (axis_size1 + axis_size2) * inner_size;

  aphrodite::mobile::parallel_for(
      outer_size, aphrodite::mobile::Thresholds::ELEMENT_WISE,
      [&](size_t start, size_t end) {
        for (size_t outer = start; outer < end; ++outer) {
          const int8_t* in1_ptr = input1 + outer * input1_stride;
          const int8_t* in2_ptr = input2 + outer * input2_stride;
          int8_t* out_ptr = output + outer * output_stride;

          size_t copy_size1 = axis_size1 * inner_size;
          std::memcpy(out_ptr, in1_ptr, copy_size1 * sizeof(int8_t));

          size_t copy_size2 = axis_size2 * inner_size;
          std::memcpy(out_ptr + copy_size1, in2_ptr,
                      copy_size2 * sizeof(int8_t));
        }
      });
}

void transpose_2d_int8(const int8_t* source, int8_t* destination,
                       size_t num_rows, size_t num_cols, size_t start_row,
                       size_t end_row) {
  constexpr size_t TILE_SIZE = 64;
  constexpr size_t VECTOR_WIDTH = 16;

  for (size_t row_tile_start = start_row; row_tile_start < end_row;
       row_tile_start += TILE_SIZE) {
    const size_t row_tile_end = std::min(row_tile_start + TILE_SIZE, end_row);

    for (size_t col_tile_start = 0; col_tile_start < num_cols;
         col_tile_start += TILE_SIZE) {
      const size_t col_tile_end =
          std::min(col_tile_start + TILE_SIZE, num_cols);

      for (size_t row_block = row_tile_start; row_block < row_tile_end;
           row_block += VECTOR_WIDTH) {
        const size_t row_block_end =
            std::min(row_block + VECTOR_WIDTH, row_tile_end);

        for (size_t col_block = col_tile_start; col_block < col_tile_end;
             col_block += VECTOR_WIDTH) {
          const size_t col_block_end =
              std::min(col_block + VECTOR_WIDTH, col_tile_end);

          if (row_block_end - row_block >= 8 &&
              col_block_end - col_block >= 8) {
            int8x8_t rows[8];
            for (int i = 0; i < 8; i++) {
              if (row_block + i < row_block_end) {
                rows[i] =
                    vld1_s8(&source[(row_block + i) * num_cols + col_block]);
              } else {
                rows[i] = vdup_n_s8(0);
              }
            }

            int8x8x2_t r01 = vtrn_s8(rows[0], rows[1]);
            int8x8x2_t r23 = vtrn_s8(rows[2], rows[3]);
            int8x8x2_t r45 = vtrn_s8(rows[4], rows[5]);
            int8x8x2_t r67 = vtrn_s8(rows[6], rows[7]);

            int16x4x2_t r0123_low = vtrn_s16(vreinterpret_s16_s8(r01.val[0]),
                                             vreinterpret_s16_s8(r23.val[0]));
            int16x4x2_t r0123_high = vtrn_s16(vreinterpret_s16_s8(r01.val[1]),
                                              vreinterpret_s16_s8(r23.val[1]));
            int16x4x2_t r4567_low = vtrn_s16(vreinterpret_s16_s8(r45.val[0]),
                                             vreinterpret_s16_s8(r67.val[0]));
            int16x4x2_t r4567_high = vtrn_s16(vreinterpret_s16_s8(r45.val[1]),
                                              vreinterpret_s16_s8(r67.val[1]));

            int32x2x2_t final_0123 =
                vtrn_s32(vreinterpret_s32_s16(r0123_low.val[0]),
                         vreinterpret_s32_s16(r4567_low.val[0]));
            int32x2x2_t final_4567 =
                vtrn_s32(vreinterpret_s32_s16(r0123_low.val[1]),
                         vreinterpret_s32_s16(r4567_low.val[1]));
            int32x2x2_t final_89AB =
                vtrn_s32(vreinterpret_s32_s16(r0123_high.val[0]),
                         vreinterpret_s32_s16(r4567_high.val[0]));
            int32x2x2_t final_CDEF =
                vtrn_s32(vreinterpret_s32_s16(r0123_high.val[1]),
                         vreinterpret_s32_s16(r4567_high.val[1]));

            int8x8_t transposed[8] = {vreinterpret_s8_s32(final_0123.val[0]),
                                      vreinterpret_s8_s32(final_4567.val[0]),
                                      vreinterpret_s8_s32(final_0123.val[1]),
                                      vreinterpret_s8_s32(final_4567.val[1]),
                                      vreinterpret_s8_s32(final_89AB.val[0]),
                                      vreinterpret_s8_s32(final_CDEF.val[0]),
                                      vreinterpret_s8_s32(final_89AB.val[1]),
                                      vreinterpret_s8_s32(final_CDEF.val[1])};

            for (int col = 0; col < 8 && col_block + col < col_block_end;
                 col++) {
              if (col_block + col < num_cols) {
                vst1_s8(&destination[(col_block + col) * num_rows + row_block],
                        transposed[col]);
              }
            }
          } else {
            for (size_t row = row_block; row < row_block_end; row++) {
              for (size_t col = col_block; col < col_block_end; col++) {
                destination[col * num_rows + row] =
                    source[row * num_cols + col];
              }
            }
          }
        }
      }
    }
  }
}

void transpose_2d_f32(const float* source, float* destination, size_t num_rows,
                      size_t num_cols, size_t start_row, size_t end_row) {
  constexpr size_t TILE_SIZE = 32;
  constexpr size_t VECTOR_WIDTH = 4;

  for (size_t row_tile_start = start_row; row_tile_start < end_row;
       row_tile_start += TILE_SIZE) {
    const size_t row_tile_end = std::min(row_tile_start + TILE_SIZE, end_row);

    for (size_t col_tile_start = 0; col_tile_start < num_cols;
         col_tile_start += TILE_SIZE) {
      const size_t col_tile_end =
          std::min(col_tile_start + TILE_SIZE, num_cols);

      for (size_t row_block = row_tile_start; row_block < row_tile_end;
           row_block += VECTOR_WIDTH) {
        const size_t row_block_end =
            std::min(row_block + VECTOR_WIDTH, row_tile_end);

        for (size_t col_block = col_tile_start; col_block < col_tile_end;
             col_block += VECTOR_WIDTH) {
          const size_t col_block_end =
              std::min(col_block + VECTOR_WIDTH, col_tile_end);

          if (row_block_end - row_block >= 4 &&
              col_block_end - col_block >= 4) {
            float32x4_t rows[4];
            for (int i = 0; i < 4; i++) {
              if (row_block + i < row_block_end) {
                rows[i] =
                    vld1q_f32(&source[(row_block + i) * num_cols + col_block]);
              } else {
                rows[i] = vdupq_n_f32(0.0f);
              }
            }

            float32x4x2_t r01 = vtrnq_f32(rows[0], rows[1]);
            float32x4x2_t r23 = vtrnq_f32(rows[2], rows[3]);

            float32x4_t col0 = vcombine_f32(vget_low_f32(r01.val[0]),
                                            vget_low_f32(r23.val[0]));
            float32x4_t col1 = vcombine_f32(vget_low_f32(r01.val[1]),
                                            vget_low_f32(r23.val[1]));
            float32x4_t col2 = vcombine_f32(vget_high_f32(r01.val[0]),
                                            vget_high_f32(r23.val[0]));
            float32x4_t col3 = vcombine_f32(vget_high_f32(r01.val[1]),
                                            vget_high_f32(r23.val[1]));

            if (col_block + 0 < num_cols) {
              if (row_block_end - row_block >= 4) {
                vst1q_f32(&destination[(col_block + 0) * num_rows + row_block],
                          col0);
              } else {
                float temp[4];
                vst1q_f32(temp, col0);
                for (size_t i = 0; i < row_block_end - row_block; ++i) {
                  destination[(col_block + 0) * num_rows + row_block + i] =
                      temp[i];
                }
              }
            }
            if (col_block + 1 < num_cols) {
              if (row_block_end - row_block >= 4) {
                vst1q_f32(&destination[(col_block + 1) * num_rows + row_block],
                          col1);
              } else {
                float temp[4];
                vst1q_f32(temp, col1);
                for (size_t i = 0; i < row_block_end - row_block; ++i) {
                  destination[(col_block + 1) * num_rows + row_block + i] =
                      temp[i];
                }
              }
            }
            if (col_block + 2 < num_cols) {
              if (row_block_end - row_block >= 4) {
                vst1q_f32(&destination[(col_block + 2) * num_rows + row_block],
                          col2);
              } else {
                float temp[4];
                vst1q_f32(temp, col2);
                for (size_t i = 0; i < row_block_end - row_block; ++i) {
                  destination[(col_block + 2) * num_rows + row_block + i] =
                      temp[i];
                }
              }
            }
            if (col_block + 3 < num_cols) {
              if (row_block_end - row_block >= 4) {
                vst1q_f32(&destination[(col_block + 3) * num_rows + row_block],
                          col3);
              } else {
                float temp[4];
                vst1q_f32(temp, col3);
                for (size_t i = 0; i < row_block_end - row_block; ++i) {
                  destination[(col_block + 3) * num_rows + row_block + i] =
                      temp[i];
                }
              }
            }
          } else {
            for (size_t row = row_block; row < row_block_end; row++) {
              for (size_t col = col_block; col < col_block_end; col++) {
                destination[col * num_rows + row] =
                    source[row * num_cols + col];
              }
            }
          }
        }
      }
    }
  }
}

void transpose_int8(const int8_t* source, int8_t* destination,
                    const size_t* shape, const size_t* permutation, size_t ndim,
                    size_t start_idx, size_t end_idx) {
  if (ndim == 2 && permutation[0] == 1 && permutation[1] == 0) {
    size_t num_rows = shape[0];
    size_t num_cols = shape[1];

    constexpr size_t THRESHOLD = 8192;
    constexpr size_t TILE_ROWS = 64;
    if (num_rows * num_cols >= THRESHOLD) {
      const size_t num_row_blocks = (num_rows + TILE_ROWS - 1) / TILE_ROWS;

      aphrodite::mobile::parallel_for(
          num_row_blocks, aphrodite::mobile::Thresholds::SCALAR_EXPENSIVE,
          [=](size_t start_block, size_t end_block) {
            for (size_t block_idx = start_block; block_idx < end_block;
                 ++block_idx) {
              size_t start_row = block_idx * TILE_ROWS;
              size_t end_row = std::min(start_row + TILE_ROWS, num_rows);

              transpose_2d_int8(source, destination, num_rows, num_cols,
                                       start_row, end_row);
            }
          });
    } else {
      cactus_transpose_2d_int8(source, destination, num_rows, num_cols, 0,
                               num_rows);
    }
  } else {
    for (size_t idx = start_idx; idx < end_idx; ++idx) {
      size_t src_idx = 0;
      size_t tmp_idx = idx;

      for (size_t i = 0; i < ndim; ++i) {
        size_t coord = tmp_idx % shape[permutation[ndim - 1 - i]];
        tmp_idx /= shape[permutation[ndim - 1 - i]];

        size_t stride = 1;
        for (size_t j = permutation[ndim - 1 - i] + 1; j < ndim; ++j) {
          stride *= shape[j];
        }
        src_idx += coord * stride;
      }

      destination[idx] = source[src_idx];
    }
  }
}
}  // namespace aphrodite::mobile
